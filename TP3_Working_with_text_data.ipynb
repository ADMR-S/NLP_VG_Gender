{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9f8750",
   "metadata": {
    "id": "1c9f8750"
   },
   "source": [
    "# **Working with Text Data**\n",
    "\n",
    "This notebook is based on the tutorial Scikit-learn « Working with Text Data ». The goal of this guide is to explore some of the main scikit-learn tools on a single practical task: analyzing a collection of text documents (newsgroups posts) on twenty different topics.\n",
    "we will see how to: load the file contents and the categories, extract feature vectors suitable for machine learning, train a linear model to perform categorization, use a grid search strategy to find a good configuration of both the feature extraction components and the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d31659",
   "metadata": {
    "id": "98d31659"
   },
   "source": [
    "# **1. Installation nécessaires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea4e280",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ea4e280",
    "outputId": "bef781b6-b657-40d0-fd40-09115906d088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\adam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\adam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\adam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\adam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\adam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed05b59",
   "metadata": {
    "id": "4ed05b59"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0492f",
   "metadata": {
    "id": "c0a0492f"
   },
   "source": [
    "# **2. Dataset**\n",
    "The dataset `fetch_20newsgroups` is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1tfOtUTizIsl",
   "metadata": {
    "id": "1tfOtUTizIsl"
   },
   "outputs": [],
   "source": [
    "# In order to get faster execution times for this first example, we will work on a partial dataset with only 4 categories out of the 20 available in the dataset:\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "V8HuZKNazP6T",
   "metadata": {
    "id": "V8HuZKNazP6T"
   },
   "outputs": [],
   "source": [
    "# We can now load the list of files matching those categories as follows:\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "v9UHxspvza57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v9UHxspvza57",
    "outputId": "b4452fe7-f086-4de8-b9e1-1fe6234a9e15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset length\n",
    "len(newsgroups_train.data) # len(newsgroups.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "KG4mHPxx0del",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KG4mHPxx0del",
    "outputId": "60c5575f-a2a4-4d45-913e-6cda265e2cd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categories\n",
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4rix2940AG6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4rix2940AG6",
    "outputId": "e9727cca-ee21-4136-e8b5-dec28a04e151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: sd345@city.ac.uk (Michael Collier)',\n",
       " 'Subject: Converting images to HP LaserJet III?',\n",
       " 'Nntp-Posting-Host: hampton',\n",
       " 'Organization: The City University',\n",
       " 'Lines: 14',\n",
       " '',\n",
       " 'Does anyone know of a good way (standard PC application/PD utility) to',\n",
       " 'convert tif/img/tga files into LaserJet III format.  We would also like to',\n",
       " 'do the same, converting to HPGL (HP plotter) files.',\n",
       " '',\n",
       " 'Please email any response.',\n",
       " '',\n",
       " 'Is this the correct group?',\n",
       " '',\n",
       " 'Thanks in advance.  Michael.',\n",
       " '-- ',\n",
       " 'Michael Collier (Programmer)                 The Computer Unit,',\n",
       " 'Email: M.P.Collier@uk.ac.city                The City University,',\n",
       " 'Tel: 071 477-8000 x3769                      London,',\n",
       " 'Fax: 071 477-8565                            EC1V 0HB.',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first one\n",
    "newsgroups_train.data[0].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hM0Zx6xz0vXH",
   "metadata": {
    "id": "hM0Zx6xz0vXH"
   },
   "source": [
    "Les algorithmes d'apprentissage supervisé ont besoin d'une étiquette de catégorie pour chaque document de l'ensemble de formation.\n",
    "\n",
    "Pour des raisons de rapidité et d'efficacité, scikit-learn charge l'attribut target sous la forme d'un tableau d'entiers correspondant à l'index du nom de la catégorie dans la liste target_names. L'identifiant entier de la catégorie de chaque échantillon est stocké dans l'attribut target :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "t2KezgdK1osn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2KezgdK1osn",
    "outputId": "dd9a9036-35d6-427d-8de6-6f7f30bac1d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "# To get category names\n",
    "for t in newsgroups_train.target[:10]:\n",
    "  print(newsgroups_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "yYsQlVcs0Xfx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYsQlVcs0Xfx",
    "outputId": "9a60a106-095d-47d3-cf41-ab514269efb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catégorie du premier exemple:  1\n",
      "catégories de notre sous-ensemble:  ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "catégorie du premier exemple traduit:  comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print(\"catégorie du premier exemple: \", newsgroups_train.target[0])\n",
    "print(\"catégories de notre sous-ensemble: \", newsgroups_train.target_names)\n",
    "print(\"catégorie du premier exemple traduit: \", newsgroups_train.target_names[newsgroups_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MQvVgoL91QAa",
   "metadata": {
    "id": "MQvVgoL91QAa"
   },
   "source": [
    "**Exercice : choisissez une autre catégorie et explorez ses exemples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ezrPcqLb1Phv",
   "metadata": {
    "id": "ezrPcqLb1Phv"
   },
   "outputs": [],
   "source": [
    "### écrire ici !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4e5a1-05c0-4a01-8b98-696f3a014c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb4af7-1b6d-4ac7-bd87-0a333d291b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5601c1a4",
   "metadata": {
    "id": "5601c1a4"
   },
   "source": [
    "# **3. Extracting features from text files**\n",
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0rQEtg2A89",
   "metadata": {
    "id": "0a0rQEtg2A89"
   },
   "source": [
    "## **3.1 Bag of Words (BOW)**\n",
    "The most intuitive way to do so is to use a bags of words representation:\n",
    "For each document #i, \n",
    "\n",
    "1. Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "2. For each document `#i`, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary.\n",
    "\n",
    "The bags of words representation implies that n_features is the number of distinct words in the corpus: this number is typically larger than 100,000.\n",
    "\n",
    "If `n_samples` == 10000, storing `X` as a NumPy array of type float32 would require 10000 x 100000 x 4 bytes = 4GB in RAM which is barely manageable on today’s computers.\n",
    "\n",
    "Fortunately, most values in X will be zeros since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words **are typically high-dimensional sparse datasets**. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.\n",
    "\n",
    "`scipy.sparse` matrices are data structures that do exactly this, and scikit-learn has built-in support for these structures.\n",
    "\n",
    "Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85793344",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85793344",
    "outputId": "b779238a-671b-43bf-fdfe-26ddde30ad4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the bag-of-words matrix: (2257, 35788)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(newsgroups_train.data)\n",
    "\n",
    "print(\"Shape of the bag-of-words matrix:\", X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kO0w1fWi3BhT",
   "metadata": {
    "id": "kO0w1fWi3BhT"
   },
   "source": [
    "CountVectorizer supports counts of N-grams of words or consecutive characters. Once fitted, the vectorizer has built a dictionary of feature indices:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "s83xV4Uj29bG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s83xV4Uj29bG",
    "outputId": "a3dff2f7-c26d-4e3a-abb6-78b56346573e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sVUzCROP3EhA",
   "metadata": {
    "id": "sVUzCROP3EhA"
   },
   "source": [
    "La valeur de l'indice d'un mot dans le vocabulaire est liée à sa fréquence dans l'ensemble du corpus de formation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df373236",
   "metadata": {
    "id": "df373236"
   },
   "source": [
    "## **3.2 Using TF-IDF Transformer**\n",
    "\n",
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using TfidfTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfb617da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfb617da",
    "outputId": "3d723630-17db-4114-dbd5-04176992e3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF matrix: (2257, 35788)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# adapter notre estimateur aux données\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "\n",
    "# transformer notre matrice de comptage en une représentation tf-idf\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "print(\"Shape of the TF-IDF matrix:\", X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "iWrngte-5MI5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWrngte-5MI5",
    "outputId": "4439d5ff-3fbf-41ba-995c-bffd5f253ae8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nous pouvons également faire ceci\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4867e26",
   "metadata": {
    "id": "f4867e26"
   },
   "source": [
    "# **4. Training a classifier**\n",
    "Now that we have our features, we can train a classifier to try to predict the category of a post. Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier, and the one most suitable for word counts is the multinomial variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "GPNHJNi84JrJ",
   "metadata": {
    "id": "GPNHJNi84JrJ"
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tf, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9JQsoA4rUD",
   "metadata": {
    "id": "2a9JQsoA4rUD"
   },
   "source": [
    "To try to predict the outcome on a new document we need to extract the features using almost the same feature extracting chain as before. The difference is that we call `transform` instead of `fit_transform` on the transformers, since they have already been fit to the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "PWAvAPaz4tsl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWAvAPaz4tsl",
    "outputId": "b0641ede-c258-4720-9d06-dcb86e9ccd3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Medicine is advancing fast' => sci.med\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['Medicine is advancing fast', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, newsgroups_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zG59DPlD5gdy",
   "metadata": {
    "id": "zG59DPlD5gdy"
   },
   "source": [
    "# **5. Building a pipeline**\n",
    "\n",
    "In order to make the vectorizer => transformer => classifier easier to work with, `scikit-learn` provides a `Pipeline` class that behaves like a compound classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1543107a",
   "metadata": {
    "id": "1543107a"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "_78bauUZ6Cci",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "_78bauUZ6Cci",
    "outputId": "80bf6a6c-0d4d-45a8-8b5d-aab00a989167"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(newsgroups_train.data, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mZe-Urri6LtG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZe-Urri6LtG",
    "outputId": "049cd026-d4dc-4b68-f430-550cc7c0d1d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Medicine is advancing fast' => sci.med\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# same behaviour\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, newsgroups_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41dab7",
   "metadata": {
    "id": "dd41dab7"
   },
   "source": [
    "# **6. Model evaluation**\n",
    "Evaluating the predictive accuracy of the model is equally easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9368cd68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9368cd68",
    "outputId": "762b9314-9497-4f99-b51e-b1324d936bf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348868175765646"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "docs_test = newsgroups_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == newsgroups_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ezwmye3m7WTM",
   "metadata": {
    "id": "ezwmye3m7WTM"
   },
   "source": [
    "Let's try a different model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0yFN5ZVC7YWS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yFN5ZVC7YWS",
    "outputId": "c758c77c-72e3-421f-f2bb-b33cf940e9d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9101198402130493"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "text_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == newsgroups_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "Px8LL_7o7gjo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Px8LL_7o7gjo",
    "outputId": "f62879a5-d73b-42c9-bde7-d0bfe94aecff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report: \n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.80      0.87       319\n",
      "         comp.graphics       0.87      0.98      0.92       389\n",
      "               sci.med       0.94      0.89      0.91       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "              accuracy                           0.91      1502\n",
      "             macro avg       0.91      0.91      0.91      1502\n",
      "          weighted avg       0.91      0.91      0.91      1502\n",
      "\n",
      "confusion_matrix: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[256,  11,  16,  36],\n",
       "       [  4, 380,   3,   2],\n",
       "       [  5,  35, 353,   3],\n",
       "       [  5,  11,   4, 378]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics to evaluate the results\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"classification_report: \\n\")\n",
    "print(metrics.classification_report(newsgroups_test.target, predicted,\n",
    "    target_names=newsgroups_test.target_names))\n",
    "\n",
    "print(\"confusion_matrix: \\n\")\n",
    "metrics.confusion_matrix(newsgroups_test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f88abc",
   "metadata": {
    "id": "f8f88abc"
   },
   "source": [
    "# **7. Hyperparameter Tuning**\n",
    "We’ve already encountered some parameters such as use_idf in the `TfidfTransformer`. Classifiers tend to have many parameters as well; e.g., `MultinomialNB` includes a smoothing parameter alpha and `SGDClassifier` has a penalty parameter alpha and configurable loss and penalty terms in the objective function (see the module documentation, or use the Python help function to get a description of these).\n",
    "\n",
    "Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values. We try out all classifiers on either words or bigrams, with or without idf, and with a penalty parameter of either 0.01 or 0.001 for the linear SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf78c7bb",
   "metadata": {
    "id": "bf78c7bb"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "OVMWPAW28UfE",
   "metadata": {
    "id": "OVMWPAW28UfE"
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "uFz5WDvZ8F9I",
   "metadata": {
    "id": "uFz5WDvZ8F9I"
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4rb4lf8Y8Ie-",
   "metadata": {
    "id": "4rb4lf8Y8Ie-"
   },
   "outputs": [],
   "source": [
    "# For time reasons, only the first 400 examples are considered\n",
    "gs_clf = gs_clf.fit(newsgroups_train.data[:400], newsgroups_train.target[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dDUs7Xu28ecq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dDUs7Xu28ecq",
    "outputId": "54e70fed-e926-432d-d25b-9189ee555486"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names[gs_clf.predict(['Lighning is manufactured by Adobe'])[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IVo56Lv982LT",
   "metadata": {
    "id": "IVo56Lv982LT"
   },
   "source": [
    "The object’s `best_score_` and `best_params_` attributes store the best mean score and the parameters setting corresponding to that score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "zBBgL42I86JS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBBgL42I86JS",
    "outputId": "cef24819-492c-4071-eaaf-7674fd00fe4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9175000000000001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "NlD8LM6683xk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NlD8LM6683xk",
    "outputId": "d922fe5c-1a3b-4120-a669-dd23beacb8b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13ba99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\adam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\adam\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\adam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\adam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adam\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2j-eLcSO8-E2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "2j-eLcSO8-E2",
    "outputId": "e09987ab-1974-4de6-fb3b-94052686c24d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.302126</td>\n",
       "      <td>0.035317</td>\n",
       "      <td>0.044225</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'clf__alpha': 0.01, 'tfidf__use_idf': True, '...</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990567</td>\n",
       "      <td>0.085680</td>\n",
       "      <td>0.129244</td>\n",
       "      <td>0.012035</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__alpha': 0.01, 'tfidf__use_idf': True, '...</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.8975</td>\n",
       "      <td>0.027839</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.293910</td>\n",
       "      <td>0.034979</td>\n",
       "      <td>0.070258</td>\n",
       "      <td>0.032508</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'clf__alpha': 0.01, 'tfidf__use_idf': False, ...</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.048734</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.926672</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.105911</td>\n",
       "      <td>0.012893</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__alpha': 0.01, 'tfidf__use_idf': False, ...</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>0.7675</td>\n",
       "      <td>0.061543</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.309937</td>\n",
       "      <td>0.050483</td>\n",
       "      <td>0.054092</td>\n",
       "      <td>0.016545</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'clf__alpha': 0.001, 'tfidf__use_idf': True, ...</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.9175</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.800466</td>\n",
       "      <td>0.067817</td>\n",
       "      <td>0.143174</td>\n",
       "      <td>0.060161</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__alpha': 0.001, 'tfidf__use_idf': True, ...</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>0.023717</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.233976</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>0.046231</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'clf__alpha': 0.001, 'tfidf__use_idf': False,...</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.7925</td>\n",
       "      <td>0.031225</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.826545</td>\n",
       "      <td>0.084812</td>\n",
       "      <td>0.069015</td>\n",
       "      <td>0.015922</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__alpha': 0.001, 'tfidf__use_idf': False,...</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.302126      0.035317         0.044225        0.006861   \n",
       "1       0.990567      0.085680         0.129244        0.012035   \n",
       "2       0.293910      0.034979         0.070258        0.032508   \n",
       "3       0.926672      0.091797         0.105911        0.012893   \n",
       "4       0.309937      0.050483         0.054092        0.016545   \n",
       "5       0.800466      0.067817         0.143174        0.060161   \n",
       "6       0.233976      0.010740         0.046231        0.007493   \n",
       "7       0.826545      0.084812         0.069015        0.015922   \n",
       "\n",
       "  param_clf__alpha param_tfidf__use_idf param_vect__ngram_range  \\\n",
       "0             0.01                 True                  (1, 1)   \n",
       "1             0.01                 True                  (1, 2)   \n",
       "2             0.01                False                  (1, 1)   \n",
       "3             0.01                False                  (1, 2)   \n",
       "4            0.001                 True                  (1, 1)   \n",
       "5            0.001                 True                  (1, 2)   \n",
       "6            0.001                False                  (1, 1)   \n",
       "7            0.001                False                  (1, 2)   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf__alpha': 0.01, 'tfidf__use_idf': True, '...             0.9000   \n",
       "1  {'clf__alpha': 0.01, 'tfidf__use_idf': True, '...             0.8875   \n",
       "2  {'clf__alpha': 0.01, 'tfidf__use_idf': False, ...             0.7000   \n",
       "3  {'clf__alpha': 0.01, 'tfidf__use_idf': False, ...             0.6875   \n",
       "4  {'clf__alpha': 0.001, 'tfidf__use_idf': True, ...             0.9000   \n",
       "5  {'clf__alpha': 0.001, 'tfidf__use_idf': True, ...             0.9125   \n",
       "6  {'clf__alpha': 0.001, 'tfidf__use_idf': False,...             0.8250   \n",
       "7  {'clf__alpha': 0.001, 'tfidf__use_idf': False,...             0.8000   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0             0.8625             0.8875             0.9125             0.9000   \n",
       "1             0.8500             0.9000             0.9250             0.9250   \n",
       "2             0.7125             0.7750             0.8000             0.8250   \n",
       "3             0.7000             0.8000             0.8125             0.8375   \n",
       "4             0.9000             0.9250             0.9250             0.9375   \n",
       "5             0.8875             0.8875             0.9250             0.9500   \n",
       "6             0.7625             0.7500             0.8000             0.8250   \n",
       "7             0.8000             0.8000             0.8500             0.9000   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.8925        0.016956                4  \n",
       "1           0.8975        0.027839                3  \n",
       "2           0.7625        0.048734                8  \n",
       "3           0.7675        0.061543                7  \n",
       "4           0.9175        0.015000                1  \n",
       "5           0.9125        0.023717                2  \n",
       "6           0.7925        0.031225                6  \n",
       "7           0.8300        0.040000                5  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(gs_clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8AXORDx9iD4",
   "metadata": {
    "id": "r8AXORDx9iD4"
   },
   "source": [
    "# **8. Exercices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eKdH1MGE9nfz",
   "metadata": {
    "id": "eKdH1MGE9nfz"
   },
   "source": [
    "### **Exercice 1 : Load different categories**\n",
    "Modify the category to upload `sci.space` together with another category of your choice. Train the model and test performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "XpBZFrz-9uxr",
   "metadata": {
    "id": "XpBZFrz-9uxr"
   },
   "outputs": [],
   "source": [
    "## écrire ici !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7J2wlmGq98N_",
   "metadata": {
    "id": "7J2wlmGq98N_"
   },
   "source": [
    "### **Exercice 2 : Test features extraction**\n",
    "Test with different values of `ngram_range` in `CountVectorizer` and check the impact on the model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "MkYCJpbe-d_Q",
   "metadata": {
    "id": "MkYCJpbe-d_Q"
   },
   "outputs": [],
   "source": [
    "## écrire ici !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gFocVpYi-Ghm",
   "metadata": {
    "id": "gFocVpYi-Ghm"
   },
   "source": [
    "### **Exercice 3 : Test a different classifier**\n",
    "Replace `MultinomialNB` with `SVC` (Support Vector Classifier). Compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rVONBDXc-eYQ",
   "metadata": {
    "id": "rVONBDXc-eYQ"
   },
   "outputs": [],
   "source": [
    "## écrire ici !!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
